{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing a Model with QKeras\n",
    "\n",
    "In this section we will quantize a model using QKeras.\n",
    "\n",
    "We will start by a simple model to perform MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from qkeras import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    x = x_in = Input((784,))\n",
    "    x = Dense(20)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dense(10)(x)\n",
    "    x = Activation(\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs=x_in, outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_train_test_set():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    x_train = x_train / 256.0\n",
    "    x_test = x_test / 256.0\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "        \n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = get_train_test_set()\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, validation_split=0.1, verbose=True, batch_size=32)\n",
    "\n",
    "evaluate = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"loss = {:.6f}, accuracy = {:.4f}\".format(evaluate[0], evaluate[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a quantized model with 2 bits on the inputs and weights ($2,0,1$ means weights and bias are quantized using 2 bits, with 0 bits to the left of the decimal point, and using symmetric representations for positive and negative numbers), and with 3-bits as input to the last layer with weights and biases with 4 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qmodel():\n",
    "    x = x_in = Input((784,))\n",
    "    x = QActivation(\"quantized_relu(2)\")(x)\n",
    "    x = QDense(20,\n",
    "               kernel_quantizer=quantized_bits(2,0,1,alpha=1),\n",
    "               bias_quantizer=quantized_bits(2,0,1))(x)\n",
    "    x = QActivation(\"quantized_relu(3,1)\")(x)\n",
    "    x = QDense(10,\n",
    "               kernel_quantizer=quantized_bits(4,0,1,alpha=1),\n",
    "               bias_quantizer=quantized_bits(4,0,1))(x)\n",
    "    x = Activation(\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs=x_in, outputs=x)\n",
    "    \n",
    "    print_qstats(model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = get_qmodel()\n",
    "\n",
    "adam = Adam(lr=0.0005)\n",
    "\n",
    "qmodel.compile(optimizer=adam, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the number and types of operations for each stage of the network.\n",
    "\n",
    "In QKeras, we need to quantize all tensors (QActivation), and the weights and biases. Please note that we used an artifact to tag the input as a 2 bit input by applying $\\tt{quantized\\_relu}(2)$ which applies a quantization of 2 bits to the input and specifies that it should only keep positive numbers, thus not allocating the sign bit for the number.\n",
    "\n",
    "For weight quantization, we used $\\tt{quantized\\_bits(4,0,1,alpha=1)}$. These parameters mean 4 bits of weights, 0 bits to the left of the decimal point, and 1 means symmetric represenation for positive and negative weights. Finally, $\\tt{alpha=1}$ tells the quantizer that this representation will only have mantissa quantization. Without this parameter, QKeras will use shared exponent representation.\n",
    "\n",
    "You can see that we had to override the learning rate of Adam. Usually in a quantized network, we have to reduce the learning rate.\n",
    "\n",
    "Let's see how this network behaves now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = qmodel.fit(x_train, y_train, epochs=10, validation_split=0.1, verbose=True, batch_size=32)\n",
    "\n",
    "qmodel.save('section2_model_0.h5')\n",
    "\n",
    "evaluate = qmodel.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"loss = {:.6f}, accuracy = {:.4f}\".format(evaluate[0], evaluate[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the loss now increased (doubled), but the accuracy function reduced by roughly 3%.\n",
    "\n",
    "You should try to quantize the network by a different amount now.  For example, let's use power-of-2 quantization on the first layer so that we can do a multiplier free first layer. Just have in mind the second parameter of quantized_po2 is the maximum value (a bit different from quantized_bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qmodel():\n",
    "    x = x_in = Input((784,))\n",
    "    x = QActivation(\"quantized_relu(2)\")(x)\n",
    "    x = QDense(20,\n",
    "               kernel_quantizer=quantized_po2(4,1),\n",
    "               bias_quantizer=quantized_bits(2,0,1))(x)\n",
    "    x = QActivation(\"quantized_relu(3,1)\")(x)\n",
    "    x = QDense(10,\n",
    "               kernel_quantizer=quantized_bits(4,0,1,alpha=1),\n",
    "               bias_quantizer=quantized_bits(4,0,1))(x)\n",
    "    x = Activation(\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs=x_in, outputs=x)\n",
    "    \n",
    "    print_qstats(model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = get_qmodel()\n",
    "\n",
    "adam = Adam(lr=0.0005)\n",
    "\n",
    "qmodel.compile(optimizer=adam, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = qmodel.fit(x_train, y_train, epochs=10, validation_split=0.1, verbose=True, batch_size=32)\n",
    "\n",
    "qmodel.save('section2_model_1.h5')\n",
    "\n",
    "evaluate = qmodel.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"loss = {:.6f}, accuracy = {:.4f}\".format(evaluate[0], evaluate[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Without using any multipliers in the first layer, we were able to get pretty much the same accuracy as before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
