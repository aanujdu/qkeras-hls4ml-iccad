{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Introduction to Machine Learning/Deep Learning<BR>\n",
    "        ICCAD 2020</h1>\n",
    "</center>\n",
    "<p>\n",
    "\n",
    "<center>\n",
    "Claudionor N. Coelho Jr - Palo Alto Networks, California, USA<br>\n",
    "Sioni Summers - CERN, Geneva, Switzerland<BR>\n",
    "Vladimir Loncar - CERN, Geneva, Switzerland<BR>\n",
    "Jennifer Ngadiuba - CERN, Geneva, Switzerland<BR>\n",
    "Thea Aarrestad - CERN, Geneva, Switzerland\n",
    "</center>\n",
    "\n",
    "<p>\n",
    "This Jupyter Notebook was modified from Jupyter Notebook that was created for ICCAD 2019 by Claudionor N. Coelho Jr and Manish Pandey.\n",
    "    \n",
    "Participants familiar with Machine Learning and Deep Learning networks may skip this section of the Workshop.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this section, we will present the basics of machine learning and deep learning so that you can follow up this material and learn even more in the future.\n",
    "\n",
    "You probably have heard about Artificial Intelligence, Machine Learning and Deep Learning. How are they related? In Figure 1, we present a diagram showing their relation. Artificial Intelligence comprises of enhancing machines to exhibit some form of intelligence. Machine Learning is a branch of artificial intelligence with the property that machines can learn patterns from examples, and make decisions based on the patterns it has learned. In machine learning, the data is represented by features, which can be raw data or a function of the data (i.e. derivative of some variable). In several cases, features are computed from raw data. deep learning is a branch of machine learning, where we let the machine learn the features from raw data.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/introduction.png\" alt=\"Artificial Intelligence/Machine Learning/Deep Learning\" style=\"width:50%\">\n",
    "  <figcaption>Figure 1 - Artificial Intelligence/Machine Learning/Deep Learning </figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "In very simple terms, we are trying to learn a function or process that takes $N$ variables represented by $x$ and convert it into $M$ outputs represented by variable $y$.\n",
    "\n",
    "$$\n",
    "y = f(x)\n",
    "$$\n",
    "\n",
    "Since we do not know a priori function $f$, we try to estimate it using some form of approximation by sampling $x$ from real world experiments.\n",
    "\n",
    "If both $x$ and $y$ are available from our samples, we use supervised learning. In supervised learning, we supply to the learning algorithm both $x$ and corresponding $y$. If the result is a continuous function, we call the learning process a _regression_. If the output function we are trying to learn is discrete, we call the learning process a _classification_.\n",
    "\n",
    "In several cases, we have a very large amount of data samples, but they have not been previously labeled (i.e. we have $x$ but not $y$), for example, when we have a large number of images, but we do not have a description of what is contained in the images. However, we still want to learn the structure of the data. In such cases, we can use unsupervised learning techniques. Some of the algorithms for unsupervised learning are k-means clustering, principal component analysis, and autoencoders.\n",
    "\n",
    "This workshop is divided in the following sections. First, we will introduce you on how to read and prepare the data to use machine learning and/or deep learning. We will follow that with a brief introduction to linear regression techniques, logistic regression and the need to go deeper with deep convolutional networks. We will then present you with CNNs (Convolutional Neural Networks) and RNN (Recurrent Neural Networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you are going to use machine learning or deep learning, you will spend most of the time preparing and visualizing data.\n",
    "\n",
    "This section is divided as follows. How to read datasets, most notably CSV (comma separated values), how to visualize data sets, and how to use principal component analysis to reduce the dimensionality of data.\n",
    "\n",
    "We will first import the libraries we will use throughout this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import csv\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import decomposition\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading MNIST Dataset\n",
    "\n",
    "We will first read in MNIST dataset in this Workshop, and we will perform data preparation by visualizing this dataset, and by performing scaling of the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "_, height, width = x_train.shape\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to visualize the data or a sample of the dataset in case there are too many samples, so that we can see some emerging patterns upfront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize a random image and print its label\n",
    "index = np.random.randint(len(x_train))\n",
    "plt.title(str(y_train[index]))\n",
    "plt.imshow(x_train[index], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first operation we perform on this dataset is to convert the output from a single output to a categorical output.\n",
    "\n",
    "In this initial model, we will be using Dense networks, so the input shape we will be using has the shape $(B, H*W)$, where $B$ is the number of images on our training set, $H$ is the height of the image, $W$ is the width of the image, so we will collapse the height and width of the image. Later on, when we use convolutional networks, we will reshape the input image as $(B, H, W, C)$, where $B$, $H$ and $W$ are the same as before, and $C$ is the number of channels. So, for a black and white image, $C = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to convert the output to categorical data\n",
    "\n",
    "y_train_c = to_categorical(y_train, num_classes=n_classes)\n",
    "y_test_c = to_categorical(y_test, num_classes=n_classes)\n",
    "\n",
    "# for this first experiment, we want to use a dense network\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], height * width)).astype(np.float32)\n",
    "x_test = x_test.reshape((x_test.shape[0], height * width)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Variables\n",
    "\n",
    "When the dimension of $x$ is greater than 1, i.e. $N > 1$, it is often the case that variables have a very different ranges, as we have seen in the wine dataset, which may impact the analysis of the results, as we will see later. For example, when computing the correlation matrix, if variable ranges are too different, we may run into stability issues. \n",
    "\n",
    "Without going too much in the discussion at this time on why we need to scale the variables, we will give you a brief intuition on the subject.  In Figure 1, suppose we move from bottom to top on the y-axis of the ellipsis on the left. You can easily see that doing a full swing on the vertical axis corresponds only to going half way on the horizontal axis. On the other hand, on the circle, a full swing on circle on the horizontal axis of the circle corresponds to the same full swing on the vertical axis.\n",
    "\n",
    "When we start running iteractive algorithms to train the network, if variables are unscaled, and as a result have different ranges between the minimum and maximum allowed values, we run the risk of doing a small swing on one of the axis, but performing a full swing on the other axis, which may take the second variable outside allowed ranges. As a result, we may reach either suboptimal results, or even run into numeric stability problems.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/circles.png\" alt=\"Unscaled and scaled 2d relations\" style=\"width:50%\">\n",
    "  <figcaption>Figure 1 - Unscaled and scaled 2d relations </figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "There is no rule of thumb, and some people prefer to scale a variable using the mean and standard deviation, but a lot of times, it is good to keep variables scaled between $[-0.5,0.5]$, $[-1,+1]$ or $[0,1]$.  If you have any variable $x_i$, to scale this variable between 0 and +1, we can use the following formula.\n",
    "\n",
    "$$\n",
    "x_i' = \\frac{x_i - \\mathtt{min}(x_i)}{\\mathtt{max}(x_i) - \\mathtt{min}(x_i)}\n",
    "$$\n",
    "\n",
    "In case we want to scale variable $x_i$ between -0.5 and +0.5, we need to make the center point between $\\mathtt{max}(x_i)$ and $\\mathtt{min}(x_i)$ to be 0.\n",
    "\n",
    "$$\n",
    "x_i' = \\frac{x_i - \\mathtt{min}(x_i)}{\\mathtt{max}(x_i) - \\mathtt{min}(x_i)} - 0.5\n",
    "$$\n",
    "\n",
    "The input images can be scaled between 0 and 1 using sklearn preprocessing capability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = MinMaxScaler(feature_range=(-0.5,0.5))\n",
    "x_scaler.fit(x_train)\n",
    "\n",
    "x_train = x_scaler.transform(x_train)\n",
    "x_test = x_scaler.transform(x_test)\n",
    "print(pd.DataFrame(x_train).min()[350], pd.DataFrame(x_train).max()[350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn preprocessing capability is very convenient because it enable us to quickly recover the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = x_scaler.inverse_transform(x_train)\n",
    "print(pd.DataFrame(example).min()[350], pd.DataFrame(example).max()[350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the original limits of any point in MNIST are 0 and 255, i.e. they should fit into an 8-bit number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing High Dimensional Datasets\n",
    "\n",
    "There are several techniques to visualize high dimensional datasets. \n",
    "\n",
    "It is always good to visualize the histogram distribution of the data.  We can visualize a flattened view of all pixels of our training set, or visualizing the histogram of a given pixel.  In this case, let's visualize a pixel in the middle of the image, as most of the pixels in the corner are dark (have value 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(x_train[:, 350])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have shown previously that we can plot the histogram of each of the variables to understand how well distributed the samples are.\n",
    "\n",
    "We can plot each $x_i$ variable and output variable $y_k$ in a 2d plot.  We can also combine several of the input variables to a 2d space, for example, uing PCA (Principal Component Analysis) or SVD (Singular Value Decomposition), and plot them. This is specially interesting if $y_k$ is a discrete variable, as we can map each class to a different color.\n",
    "\n",
    "There are other approaches beyond PCA and SVD, for example, we can use use a Linear Discriminant Analysis (LDA), which uses the labels to try to separate the classes instead of spreading the input dataset, thus providing better results in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(xv, yv, sample_size=500):\n",
    "    \"\"\"\n",
    "    Obtains a random sample (xv, yv) of sample_size values.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    xv = xv.copy()\n",
    "    yv = yv.copy()\n",
    "    \n",
    "    sample = np.arange(xv.shape[0])\n",
    "\n",
    "    np.random.shuffle(sample)\n",
    "\n",
    "    sample = sample[0:sample_size]\n",
    "\n",
    "    xv = xv[sample]\n",
    "    yv = yv[sample]\n",
    "    \n",
    "    return xv, yv\n",
    "\n",
    "def plot_visualize(xv, yv, n_classes=2, centroid=None):\n",
    "    \"\"\"\n",
    "    Plots 2d values for each color given by the class of yv.\n",
    "    \"\"\"\n",
    "    \n",
    "    if xv.shape[1] == 1:\n",
    "        return\n",
    "    for _class in range(n_classes):\n",
    "        mask = (yv == _class)\n",
    "        \n",
    "        # if we do not have any samples of mask, there is nothing to plot\n",
    "        if np.sum(mask) == 0:\n",
    "            continue\n",
    "\n",
    "        plt.plot(xv[mask, 0], xv[mask, 1], 'o', label=\"class = {}\".format(_class))\n",
    "        \n",
    "    if centroid is not None:\n",
    "        for _class in range(n_classes):\n",
    "            plt.plot(centroid[_class, 0], centroid[_class, 1], 'x', color=\"k\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "USE_LDA = 1\n",
    "\n",
    "xv, yv = resample(x_train, y_train, sample_size=1000)\n",
    "n_classes = 10\n",
    "\n",
    "if USE_LDA:\n",
    "    lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "    lda.fit(xv, yv)\n",
    "    transform = lda.transform\n",
    "else:\n",
    "    pca = decomposition.PCA(n_components=2)\n",
    "    pca.fit(xv)\n",
    "    transform = pca.transform\n",
    "\n",
    "plot_visualize(xv=transform(xv), yv=yv, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#maaten_hinton_tsne\">Maaten et al</a> suggested using TSNE to visualize high-dimensional data, but if the dimensionality of the data is > 50, we should first use PCA or SVD to reduce it to 50. We are presenting the code here.  Even if the reader browse through the help on [manifold embedding](http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py), the reader will see several different examples on how to fold multi-dimensional space into 2d space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xv, yv = resample(x_train, y_train, sample_size=1000)\n",
    "plot_visualize(xv=TSNE(n_components=2).fit_transform(xv), yv=yv, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this section, we showed how to read MNIST dataset, and we then showed informally the importance of scaling variables using sklearn preprocessing pipeline, and showed how to reduce the dimensionality of the problem by using PCA, LDA and TNSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Validation-Test Set\n",
    "\n",
    "For complex problems, the number of samples we have is far from complete, so we want to know how our model behaves when it estimates the value of the output function (predict) with new data.\n",
    "\n",
    "Usually, we separate our dataset into three sets. \n",
    "\n",
    "The train set is the set we will use to train the model. The validation set is a set that helps us identify when we have trained the model enough (to avoid the model from learning too well how to beat the training set, but not some data it has not seen before - called overfitting), and to tune the training for hyperparameters (such as the model architecture, batch size, number of features  or variables, learning rate of the gradient descent algorithm, etc). The test set is the final test set where we will give a performance metric, and it should not be mixed with the training and validation sets.\n",
    "\n",
    "Minimally, the validation and the test set should come from the same distribution.  It is not uncommon for people to use the same validation and test set, but they should not be the same in general, as we would be fine tuning the model for the validation set, and we cannot determine how the model will behave when we present new data it has not seen before.\n",
    "\n",
    "We will use Keras to split the train and validation test in the following runs, by using the 90% of our dataset to belong to the training set, and 10% of our dataset to belong to the validation set.  Usually, using validation and test sets of 10,000 cases for very large training sets is enough.  For smaller training sets, we need a larger percentage of the training set allocated as validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
    "y_train_c = to_categorical(y_train, num_classes=n_classes)\n",
    "y_valid_c = to_categorical(y_valid, num_classes=n_classes)\n",
    "print(x_train.shape[0], \"training images\", x_valid.shape[0], \"validation images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use Keras Dense model to try to classify the images. Do not worry if you do not understand every part of the code, as it will be explained later.\n",
    "\n",
    "We are using Adam to train this network, and we will use categorical crossentropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential(name=\"sequential\")\n",
    "model.add(Dense(n_classes, name=\"d2\", activation=\"softmax\", input_shape=x_train.shape[1:]))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Layer\n",
    "\n",
    "A Dense layer performs the following operation $y = f(x) = W x + b$, where $W$ and $b$, so the function computing a cutting plane for each one of the output classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Classification\n",
    "\n",
    "When we are classifying a variable that may take several different values such as in the MNIST case (let's call each output variable $y_m$ to have $C_m$ classes), we have two options. We can create $C_m$ binary variables and label each output $y_m == m$ to be 1, and 0 otherwise; or we can use a $\\mathtt{softmax}$ function. A softmax function performs the operation $\\mathtt{softmax}(z) = \\frac{e^{z_j}}{\\sum_j e^{z_j}}$. This last option is what we will chose in the model defined above.\n",
    "\n",
    "Let's see how this model train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train_c, batch_size=64, epochs=30,\n",
    "    validation_data=(x_valid, y_valid_c),\n",
    "    shuffle=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label=\"acc\")\n",
    "plt.plot(history.history['val_accuracy'], label=\"val_acc\")\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label=\"loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"val_loss\")\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph, you can see that we should expect our model to respond to a new dataset with the same statistical distribution as our validation set with an accuracy close to 93%.\n",
    "\n",
    "In addition, in the normal case, we should expect the validation loss to be higher than the training loss, which is not the case here. There are several explanations for that, and usually this can be explained by removing any randomization that occurs during the training process, such as it is the case of shuffling the input set that randomly samples the training set into chuncks of _batch\\_size_. \n",
    "\n",
    "Finally, beyond looking for randomizations happening inside the training process, any oscillation on the training loss is also an indication we may have a learning rate that is too big.\n",
    "\n",
    "Usually we use the validation set to tune hyperparameters such as learning rate, loss function and other hyperparameters such as network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Deeper\n",
    "\n",
    "Let's consider we are designing a ML model that classifies breeds of dogs, so that $x$ is an image of a dog, and $y$ is its breed. In Figure 2, we can see that our function $f$ maps images into breeds of dogs classes.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/DogClassifier.png\" alt=\"Dog Classifier\" style=\"width:60%\">\n",
    "  <figcaption>Figure 2 - Dog Classifier </figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "The problem comes when our pictures are not as clean as the ones presented in Figure 2. In Figure 3, we can see that it is hard to define how our classifier should operate.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/TwoDogs.png\" alt=\"Two Dogs\" style=\"width:60%\">\n",
    "  <figcaption>Figure 3 - Hard Classification Problem </figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "Because it is hard to define the boundaries of the dog we want to classify, we try to extract features from the image, like, a dog has two eyes, two ears, one mouth, one nose, the fur may have a color (black, brown, beige, etc), so that in Figure 4, if we feed a picture and we are able to extract the features of the dog for which we want to know the breed, the answer is simple, as given in Figure 4.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/Terrier.png\" alt=\"Feature based ML\" style=\"width:70%\">\n",
    "  <figcaption>Figure 4 - Feature Based Machine Learning </figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "With enough testcases, we can improve our system to detect objects that it has not seen before, such as the dog in Figure 5.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/Chiwawa.png\" alt=\"Dog Detection\" style=\"width:60%\">\n",
    "  <figcaption>Figure 5 - Chiwawa Detection </figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "Unfortunately, no matter how hard we engineer features for our model, it will never be able to  cover all possible variations, so that even the simple muffin of Figure 6 will be able to  fool the ML system, as it seems to have two eyes, two ears, a mouth, a nose, it is beige, and it will probably be mistakenly identified as a Chiwawa.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/Muffin.png\" alt=\"Dog Detection\" style=\"width:20%\">\n",
    "  <figcaption>Figure 6 - Muffin </figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "What we need is a network that can learn not only how to classify features but to extract features. Because a human can think of a limited number of features, this network should be able compute several features and combine them layer after layer. The initial layers will detect simple features, perhaps slanted lines, with more complex features being detected later on. \n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/DeepNetwork.png\" alt=\"DN\" style=\"width:60%\">\n",
    "  <figcaption>Figure 7 - Deep Network trained on ImageNet from <a href=\"#zeiler_fergus\">Zeiler and Fergus 2013</a></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "So, in summary, we want to embed the function that generates the features $x$ into our ML model, for some raw data $r$ so that $x = g(r)$.\n",
    "\n",
    "What Figure 7 represents is an embedding of the feature extraction $x = g(r)$ into our network model, so that our network becomes more robust and immune to changes. This new model now has two parts: a feature extraction part ($x = g(r)$) and a feature classifier $y = f(x)$.  We call this network a Deep Neural Network.\n",
    "\n",
    "There are caveats to this new model.\n",
    "\n",
    "1. End-to-end network may be harder to converge than breaking the network into smaller sub-problems;\n",
    "\n",
    "2. We may know beforehand some relation between $r$ and $y$ (e.g. quadratic on some input raw variables), and as a result, we should feed this information to the network because the network model is able to abstract a non-linear behavior by approximating the function by parts. So, it will always be more precise to use non-linear relations instead of relying on the network to detect them (and it will be more expensive as well).\n",
    "\n",
    "As a result, we should always carefully look at 1 and 2 when we decide to use a deep neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Convolutional Neural Networks\n",
    "\n",
    "In this section, we present main layers of deep convolution neural networks, and how they are connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "\n",
    "If we consider a dense layer with $N_i$ inputs and $N_o$ outputs, we have $F_o (F_i + 1)$ parameters. This number may seem excessive if we have 100 inputs and 100 outputs, in which case we would have only for this layer 10100 parameters to be trained.  It is usually the case that an object of interest may be anywhere in the image, and as a result, most of these parameters will be attempting to detect the same feature (e.g. the face of a dog), which means it is desirable to have translation invariance of the feature being detected.\n",
    "\n",
    "So, we can imagine that instead of performing operations on all of the inputs, it makes sense for images to perform local operations on the inputs, e.g. high pass, low pass, edge detection, corner detection, etc, and then compose the pieces together to obtain.  In Figure 9, we can see that we are detecting simple relations in the first layers, and  concatenating or combining the results for the next layers.\n",
    "\n",
    "For example, if we are trying to detect wheels, we can first detect the edges of the wheels, and then combine the slopes to find a wheel, as presented in Figure 8.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/Wheel-Detection.png\" alt=\"Wheel-Detection\" style=\"width:60%\">\n",
    "  <figcaption>Figure 8 - Composable Wheel Detection</a></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "This filter has size $(K_h, K_w)$, and as a result, and for an input matrix of size $(I_H, I_W, I_C)$ where the triplet represents the height, width and the number of features (also called number of channels or filters, for example, for a RGB image, $I_C = 3$), and output matrix of size $(O_H, O_W, O_C)$, we will have $(K_h K_w I_C + 1) O_C$ parameters (where the 1 comes from the bias term), as we one filter ranging over all input channels for each output channel $O_C$.\n",
    "\n",
    "Figure 9 represents the amount of computation that we need to perform to compute the convolution for each output pixel of $(O_H, O_W, O_C)$, taken from [machinethink.net/blog/googles-mobile-net-architecture-on-iphone](machinethink.net/blog/googles-mobile-net-architecture-on-iphone).  This figure assumes $I_C = 3$, $K_h = K_w = 3$ (we usually have square filters, normally $3 \\times 3$) and $O_C = 1$.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/Conv2D.png\" alt=\"Conv2D\" style=\"width:30%\">\n",
    "  <figcaption>Figure 9 - Convolution 2D</a></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "When comparing the number of operations between a dense operation and a convolution operation, we have to assume $F_i = I_H I_W I_C$ and $F_o = O_H O_W O_C$, so ratio between the number of parameters in a dense operation and the number of parameters in a convolution operation is reduced by $\\approx \\frac{F_i F_o}{K_h K_w I_C O_C} = \\frac{I_H I_W I_C O_H O_W O_C}{K_h K_w I_C O_C} = \\frac{I_H I_W O_H O_W}{K_h K_w}$ if we ignore the bias term for the dense and Convolution operations.\n",
    "\n",
    "The other question the reader may be asking is how many multiplication operations are we performing for both the dense and the convolution layer.  If you just assume a simple multiplication algorithm that does not use <a href=\"#lavin_gray\">Winograd Convolution</a>, we have the following approximate number of multiplication operations.\n",
    "\n",
    "- Multiplications for dense operations: $F_o F_i = I_H I_W I_C O_H O_W O_C$\n",
    "- Multiplications for convolution operations: $K_h K_w I_C O_H O_W O_C$\n",
    "\n",
    "The convolution operation has three other parameters we need to consider. First, the _stride_ , which is the offset for the next filter operation on the input matrix. Second, the _padding_ , which is usually set to _valid_ or _same_ , determining whether we should consider the input matrix to be filled with 0's to perform the filtering on the edges, or whether we should ignore the filters on the edges, and only perform the operations on filters that fully enclose the inputs.  Third, the _dilation_ . If greater than 1, the filter is not applied to consecutive pixels, but to pixels that are appart by this parameter.\n",
    "\n",
    "Based on the stride $S$ and padding $P$, we can represent the relation between the input matrix and the output matrix as:\n",
    "\n",
    "$$\n",
    "O_H = \\left\\lfloor \\frac{I_H - K_h + 2 * P}{S} \\right\\rfloor + 1,\n",
    "O_W = \\left\\lfloor \\frac{I_W - K_w + 2 * P}{S} \\right\\rfloor + 1,\n",
    "$$\n",
    "\n",
    "If we disregard the padding, the output can be computed as:\n",
    "\n",
    "$$O[o_c][o_h][o_w] = \\mathtt{bias}[o_c] + \\sum_{k_h} \\sum_{k_w} \\sum_{i_c} I[i_c][S * o_h + k_h][S * o_w + k_w] * W[k_h][k_w][o_c][i_c]$$\n",
    "\n",
    "The reader may be thinking that we only have benefits by using an convolution operation instead of a dense operation. In fact, the number of multiplications is indeed reduced, but at an expense to perform more memory operations to read an element of the input matrix, as an element $I[i_c][i_h][i_w]$ will be read $K_h K_w$ times from the memory, as it is involved in the computation of that many filter operations.\n",
    "\n",
    "Another advantage of convolution operations is that it provides translation invariance. By applying the same filter everywhere on the input tensor, a convolution operation detects objects independent where the object is located in the image.\n",
    "\n",
    "In Keras, the Convolution operation is invoked as the following function, whose may parameters are included here.\n",
    "\n",
    "```python\n",
    "keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid',\n",
    "                    dilation_rate=(1, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separable Convolutions\n",
    "\n",
    "As one can see from the previous definition, a convolution operation is very good for images, but the number of multiplications may still be very large. Let's take for example the formula $K_h K_w I_C O_H O_W O_C$. This formula can be interpreted as for each output pixel of $(O_H, O_W, O_C$, we need to compute a filter of size $(K_h, K_w)$ for each input channel $I_C$.\n",
    "\n",
    "The idea of separable convolutions is to replace this matrix computation by two matrix operations. In the first one, called Depthwise convolution, we perform the filtering operation of size $(K_h, K_w)$ from the input matrix $(I_H, I_W, I_C)$ into the intermediate matrix $(O_H, O_W, I_C)$, so that we only compute the output for the same channel at this stage (note that the intermediate matrix has the same number of channels as the input matrix. The resulting computation is presented as follows.\n",
    "\n",
    "$$D[i_c][o_h][o_w] = \\sum_{k_h} \\sum_{k_w} I[i_c][S * o_h + k_h][S * o_w + k_w] * W_D[k_h][k_w][i_c]$$\n",
    "\n",
    "In the second operation, called Pointwise convolution, we perform a weighted average on the input channels for each output channel.\n",
    "\n",
    "$$O[o_c][o_h][o_w] = \\mathtt{bias}[o_c] + \\sum_{i_c} D[i_c][o_h][o_w] * W_P[o_c][i_c]$$\n",
    "\n",
    "This Separable convolution is represented graphically in Figure 10, for $I_C = 3$, $K_h = K_w = 3$ and $O_C = 1$, also taken from  [machinethink.net/blog/googles-mobile-net-architecture-on-iphone](machinethink.net/blog/googles-mobile-net-architecture-on-iphone).\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/SeparableConv2D.png\" alt=\"SeparableConv2D\" style=\"width:50%\">\n",
    "  <figcaption>Figure 10 - Separable Convolution 2D</a></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "What is the reduction on the number of multiplications from a normal convolution? Recall that the number of multiplications on a normal convolution is $K_h K_w I_C O_H O_W O_C$. The Depthwise convolution has $K_h K_w I_C O_H O_W$ operations, and the Pointwise convolution has $O_H O_W I_C O_C$ operations, so the reduction on the number of multiplications is $\\frac{K_h K_w I_C O_H O_W + O_H O_W I_C O_C}{K_h K_w I_C O_H O_W O_C} = \\frac{1}{O_C} + \\frac{1}{K_h K_w}$.  So, for an operation with 200 output layers and a filter of size $3 \\times 3$, we have  roughly only 11% of the number of multiplications when compared with a normal convolution. \n",
    "\n",
    "The reader should bear in mind that a separable convolution is an approximation for the full convolution operation, and as such, there are problems for which a separable convolution is not a good approximation.\n",
    "\n",
    "In Keras, a separable convolution operation can be invoked by:\n",
    "\n",
    "```python\n",
    "keras.layers.SeparableConv2D(filters, kernel_size, strides=(1, 1), padding='valid',\n",
    "                             dilation_rate=(1, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation\n",
    "\n",
    "We have shown in the previous sections and also in Figures 9 and 10 that we would like to cascade a number of dense or convolution in order to detect higher level objects such as wheels, people, etc. The good news is that we can do that by cascading a number of such operations.  The bad news is that cascading convolutions or dense operations without a non-linearity in between can be collapsed by a single operation. \n",
    "\n",
    "Remember that we can represent a dense or convolution operation by a matrix multiplication $y = Ax$ without loss of generality. Now assume we are cascading two consecutive operations without a non-linearity between then. Then, we can represent the output as $y = A_2 (A_1 x) = (A_2 A_1) x = A x$, using the distributive property of matrix multiplication. This shows that without a non-linear activation layer, we can trivially collapse all of the dense or convolution operations into a single matrix operation.\n",
    "\n",
    "There are several activation layers that can be used in deep learning. For intermediate layers, $\\mathtt{relu}$, $\\mathtt{leakyrelu}(x, \\alpha) = (x \\geq 0)?x:\\alpha x$ and $\\mathtt{relu6}(x) = \\mathtt{clip}(x, 0, 6)$ are the most common for intermediate operations, although $\\mathtt{relu}$ and $\\mathtt{relu6}$ have the drawback of discarding all the computed information if $x < 0$. For that reason, <a href=\"#mobilenetv2\">Sandler et al</a> proposed the use of bottlenecks to reduce the number of channels for the next operation by performing a linear combination of the channels (which can be implemented by a Conv2D operation where the filter size is (1,1)).\n",
    "\n",
    "For the final classification operation, it is common to use $\\mathtt{softmax}$ for multi-class classification and $\\mathtt{sigmoid}$ for single or two-class classification problem. Of course if we are doing a regression or continuous function approximation, the final layer will not contain an activation operation.\n",
    "\n",
    "In Keras, the activation operations can be invoked as presented below, where $\\mathtt{activation}$ can be one of the strings: \"relu\", \"softmax\", \"sigmoid\", \"tanh\", among others. For $\\mathtt{leakyrelu}$, we have to invoke it separately as it requires an additional parameter. Finally, $\\mathtt{relu6}$ does not have a native representation in Keras, but it can be computed (as seen in the definition of MobileNetv2 in Keras by the function below.\n",
    "\n",
    "```python\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def relu6(x):\n",
    "  return K.relu(x, max_value=6)\n",
    "  \n",
    "keras.layers.Activation(activation)\n",
    "keras.layers.Activation(relu6)\n",
    "keras.layers.LeakyReLU(alpha=0.3)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "We spent a great deal of time in the beginning discussing why we should scale the inputs. Now, if we have two consecutive dense or convolution operations, shouldn't we worry about normalizing the intermediate values as well?\n",
    "\n",
    "Batch normalization computes for each channel from the previous operation the running mean $\\mu_\\cal{B}$ and the standard deviation $\\sigma^2_\\cal{B}$ for a batch $\\cal{B}$. Then, it computes two parameters $\\gamma$ and $\\beta$ to normalize the inputs as seen in Figure 11, taken from <a href=\"#batchnormalization\">Ioffe and Szegedy</a>.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/BatchNormalization.png\" alt=\"BatchNormalization\" style=\"width:40%\">\n",
    "  <figcaption>Figure 11 - BatchNormalization</a></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "Batch normalization has an advantage as it can be merged in the weights and bias of the previous layer. In fact, if are using batch normalization we can turn off the bias in the convolution and dense layers by specifying the parameter $\\mathtt{use\\_bias=False}$ in these layers. If we do not set it, you will probably see that the bias is always 0 in these layers.  If you assumed the previous operation performed matrix multiplication $x_2 = W^{1,2} x_1$ and $x_3 = \\mathtt{BatchNormalization}(x_2)$ where 1, 2 and 3 represent the operation levels (or the order in which the operations as cascaded) in the network model, the output of the batch normalization can be optimized by merging the two operations as:\n",
    "\n",
    "$$\n",
    "x_3 = \\frac{\\gamma (x_2 - \\mu_{\\cal{B}})}{\\sqrt{\\sigma^2_{\\cal{B}} + \\epsilon}} + \\beta = \n",
    "      \\frac{\\gamma (W^{1,2} x_1 - \\mu_{\\cal{B}})}{\\sqrt{\\sigma^2_{\\cal{B}} + \\epsilon}} + \\beta = \\left( \\frac{\\gamma W^{1,2}}{\\sqrt{\\sigma^2_{\\cal{B}}}} \\right) x_1 +       \n",
    "\\left( \\beta - \\frac{\\gamma \\mu_{\\cal{B}}}{\\sqrt{\\sigma^2_{\\cal{B}} + \\epsilon}} \\right) = W^{1,2}_{new} x_1 + b_{new}\n",
    "$$\n",
    "      \n",
    "The first term being the new weights of the matrix, and the second term being the new bias.\n",
    "\n",
    "Whenever we want to embed a network with convolution and/or dense operations so that it uses lower precision (for example, using $\\mathtt{int8}$ to represent the floating point numbers as fixed point numbers with 8 bits), using batch normalization may pose some challenges, as we usually need to retrain the network with the new arithmetic, but before the batch normalization is merged with the previous operations.\n",
    "\n",
    "\n",
    "In Keras, the batch normalization can be invoked as using the following interface, where $\\mathtt{axis}$ is 0 for dense operations and convolution operations when channels appear first in the matrices, or -1 for convolution operations when the channels appear last.\n",
    "\n",
    "```python\n",
    "keras.layers.BatchNormalization(axis=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout is a regularization layer that provides a way to prevent the neural network from overfitting (<a href=\"#dropout\">Srivastava et al</a>).  A dropout randomly sets a fraction $\\mathtt{rate}$ of the inputs to 0, during training time.  Because of that, the network learns how to optimize the loss function by using only a subset of the inputs, making the network more adaptable.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/Dropout.png\" alt=\"Dropout\" style=\"width:50%\">\n",
    "  <figcaption>Figure 12 - Oringinal network (left) and with Dropout of 50% of the nodes (right)</a></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "Figure 12, shows two networks with cascaded dense layers. In the network on the right, we are only using 50% of the inputs in the intermediate layer to compute the output (dropout of 50%).\n",
    "\n",
    "Because of dropout, during training, we are not training the inputs $x$ with expected value $\\mathtt{E}[x]$ but instead $\\mathtt{rate} * \\mathtt{E}[x]$, as we will be dropping $\\mathtt{rate}$ of the inputs. As a result, during inference, we have to adjust the statistics of the inputs to match that during training. In order to overcome this difference, deep learning frameworks scale up the values during training by $1/\\mathtt{rate}$ so that you do not have to worry on what to do during inference.\n",
    "\n",
    "When using dropout and batch normalization, we may have some problems though. Remember that in batch normalization, the inputs are adjusted according to the statistic properties of the batch, namely, the mean and the standard deviation. Since dropout randomly drops inputs from the operation, we are basically changing the statistic properties of the values for the following operations. As such, the user should avoid using batch normalization after we use dropout operations, according to <a href=\"#batchnormalization_and_dropout\">Li et al</a>, or we need to adjust the inputs to the incoming batch normalization layers after dropout.\n",
    "\n",
    "In Keras, dropout can be invoked as follows.\n",
    "\n",
    "```python\n",
    "keras.layers.Dropout(rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "We have already seen that convolution operations reduce the dimensionality of the output tensor, especially if we use a stride greater than 1. Another way to provide dimensionality reduction is through pooling. Usually, pooling is interleaved with convolution layer, and we can perform max, average or random pooling on a window (typically 2x2) with a stride of 2, to guarantee a dimensionality reduction by 2.\n",
    "\n",
    "Pooling provides immunity against noise, and depending on the filter, it can also provide rotation invariance.\n",
    "\n",
    "In Figure 13,  taken from [http://cs231n.github.io/convolutional-networks/#pool](http://cs231n.github.io/convolutional-networks/#pool), we can see the effect of a max pooling operation on the inputs.   \n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/MaxPooling2D.png\" alt=\"MaxPooling2D\" style=\"width:50%\">\n",
    "  <figcaption>Figure 13 - MaxPooling2D</a></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "How can pooling provide rotation immunity? Suppose we are computing four different types of filters to detect if a line is in horizontal, veritical, inclined to the right or left.  If we feed the image that maximizes that filter, and perform a max pooling operation, the result will be exactly the filtered result that best matches the rotation of the input.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/HowMaxPoolingWorks.png\" alt=\"HowMaxPoolingWorks\" style=\"width:50%\">\n",
    "  <figcaption>Figure 14 - Rotation invariance in max pooling</a></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "Finally, global average pooling is an operation that performs the average over the width and height of the input matrix, returning a vector with the size of the input features, i.e. $O[i_c] = \\frac{1}{I_H I_W} \\sum_{i_h} \\sum_{i_w} I[i_c][i_h][i_w]$.  Global average pooling is used in all convolutional networks (<a href=\"#all_conv\">Springenberg et al</a>) to reduce the classification layer to the only the channels.\n",
    "\n",
    "In Keras, pooling operations can be invoked as using the following interface.\n",
    "\n",
    "```python\n",
    "keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')\n",
    "keras.layers.GlobalAveragePooling2D()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Convenient Operations\n",
    "\n",
    "We are finally ready to create a model that uses deep learning. Before that, we need to talk about a few of the layers that were left but will be necessary in order to create a model.\n",
    "\n",
    "```python\n",
    "keras.layers.Reshape(target_shape)\n",
    "keras.layers.Flatten()\n",
    "keras.layers.Concatenation(axis=-1)\n",
    "keras.layers.Add()\n",
    "```\n",
    "\n",
    "Reshape converts network from one shape into another shape, preserving the total number of elements of the input.  Reshape is usually used to convert from a flattened matrix to an image with height, width and channels.\n",
    "\n",
    "Flatten converts an image with three dimensions (height, width and channels) to a single dimension. It is usually used when you want to extract the features using a convolutional network, and perform feature classification using dense network.\n",
    "\n",
    "\n",
    "Concatenation and Add are used as merging operations.\n",
    "\n",
    "Concatenation is usually used when you have two different parallel networks and you want to combine them in the dimension $\\mathtt{axis}$.\n",
    "\n",
    "Finally, Add is used when you want to merge the two different parallel networks by adding them together instead of concatenating outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting All Together - Building Deeper Layered Networks\n",
    "\n",
    "We will show now how to put everything we have seen before to compose bigger deeper neural networks. We will start by the concept of a layer. Although keras call layers all the operations for historical reasons, and in fact we may have used the term layer interchangeably with operation in the previous sections, a layer is a \"composable\" unit with operations, and in most of the cases, it may contain a single dense or convolution operation, although in MobileNetV2, the authors have used a second convolution operation to provide a bottleneck operation.\n",
    "\n",
    "So, in Figure 15, we can see what a normal layer looks like.  The current consensus is that a non linear activation should come after batch normalization, although some authors have suggested that batch normalization should come after the non linear activation.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/Layer.png\" alt=\"Layer\" style=\"width:40%\">\n",
    "  <figcaption>Figure 15 - Composable operations forming a layer</a></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "the layers can be composed in parallel or sequential to for a feature extraction network, as seen in Figure 16.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/Network.png\" alt=\"Network\" style=\"width:60%\">\n",
    "  <figcaption>Figure 16 - Deep Learning Network</a></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "There are some tendencies we have seen in small or new networks.\n",
    "\n",
    "- Use separable convolution whenever possible for kernels bigger than 1x1,\n",
    "- Use batch normalization in feature extraction layers to improve convergence,\n",
    "- After dropout do not use batch normalization, as it shifts mean and variance between training and inference,\n",
    "- We can use 1x1 convolution operations in the feature classification layers instead using dense operations, as presented by <a href=\"#all_conv\">Springenberg et al</a>,\n",
    "- Always start with a network without batch normalization and without dropout and add them later to see if results improve, especially because batch normalization will make quantization more difficult,\n",
    "- If we use relu or relu6, we can use a convolution with filter size 1x1 without batch normalization and activation to reduce the number of features as part of the features will be 0's whenever they are negative,\n",
    "- It is worth trying to use a stride=2 in the convolution operation instead of using pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use the MNIST dataset to execute this exercise, which are samples of hand written digits stored in 28x28 gray scale matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape=(height * width,)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train_c, batch_size=64, epochs=20, shuffle=True,\n",
    "                    validation_data=(x_valid, y_valid_c), verbose=True)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label=\"acc\")\n",
    "plt.plot(history.history['val_accuracy'], label=\"val_acc\")\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label=\"loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"val_loss\")\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "metrics = model.evaluate(x_test, y_test_c, batch_size=64, verbose=1)\n",
    "print(metrics[0], metrics[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the MNIST dataset is very easy to train and with a small network, we can already have a very good performance, which is already over 95% on both the validation and test sets.  However, we are using 40k parameters \n",
    "in our network. We will try now with a small convolutional network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((x_train.shape[0], height, width, 1))\n",
    "x_valid = x_valid.reshape((x_valid.shape[0], height, width, 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], height, width, 1))\n",
    "\n",
    "kernel = (3, 3)\n",
    "strides = (2, 2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, kernel, strides=strides, padding='same', input_shape=(height, width, 1)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(16, kernel, strides=strides, padding='same'))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(24, kernel, strides=strides, padding='same'))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train_c, batch_size=64, epochs=10, shuffle=True,\n",
    "                    validation_data=(x_valid, y_valid_c), verbose=True)\n",
    "\n",
    "metrics = model.evaluate(x_test, y_test_c, batch_size=64, verbose=1)\n",
    "print(metrics[0], metrics[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 5x less parameters, we obtained the same performance, or 1% better. The reader may be asking why the first one seemed to train faster than the second one? Well, we only had a big matrix multiplication in the first network, and on the second one, we have smaller matrix multiplications. How many multiplications do we have in both cases? \n",
    "\n",
    "Finally, let us inspect some of the results to see how we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(x_test)\n",
    "labels = np.argmax(p, axis=-1)\n",
    "for i in range(3):\n",
    "    idx = np.random.randint(y_test.shape[0])\n",
    "    plt.imshow(x_test[idx].reshape(28, 28), cmap=\"gray\")\n",
    "    plt.title(\"Predicted label is \" + str(labels[idx]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We have shown in this section the building blocks used to build deep neural networks with application to the classification of MNIST dataset. Of course, MNIST dataset is a very easy dataset to get high accuracy, and you should classify images from cifar10 in the rest of the time you have in this workshop.\n",
    "\n",
    "Right now, for more difficult problems, networks such as <a href=\"#googlenet\">GoogleNet</a> or <a href=\"#resnet\">Resnet</a> are used, which contains many more layers, as seen in Figure 19. \n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "  <img src=\"files/GoogleNet.png\" alt=\"GoogleNet\" style=\"width:80%\">\n",
    "  <figcaption>Figure 19 - GoogleNet</a></figcaption>\n",
    "</figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "cifar10_labels = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \n",
    "    \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "for i in range(3):\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.title(cifar10_labels[y_train[i][0]])\n",
    "    plt.show()\n",
    "\n",
    "# now it is your turn to create a network to classify cifar10 images\n",
    "# remember cifar10 has input size (32, 32, 3)\n",
    "\n",
    "# your code begins here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this section, we introduced the reader to the layers deep neural netowrks, and with this information, the reader should be able to create small deep neural networks.\n",
    "\n",
    "We tried to steer away from statistical details, trying to focus on ML/DL as an optimization problem, showing the readers an intuition on why things are done in a certain way.\n",
    "\n",
    "This should give you enough information to start using ML/DL, getting gratification from and and start looking for additional resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "\n",
    "<a id=\"maaten_hinton_tsne\"></a>[1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data\n",
    "Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.\n",
    "\n",
    "<a id=\"zeiler_fergus\"></a>[2] Zeiler, M.D.; Fergus, R. Visualizing and Understanding Convolutional Networks. [https://arxiv.org/abs/1311.2901](https://arxiv.org/abs/1311.2901)\n",
    "\n",
    "<a id=\"lavin_gray\"></a>[3] Lavin, A.; Gray, S. Fast Algorithms for Convolutional Neural Networks. [https://arxiv.org/abs/1509.09308](https://arxiv.org/abs/1509.09308)\n",
    "\n",
    "<a id=\"mobilenetv2\"></a>[4] Sandler, M.; Howard, A.; Zhu, M.; Zhmoginov, A.; Chen, L.-C. MobileNetV2: Inverted Residuals and Linear Bottlenecks. [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)\n",
    "\n",
    "<a id=\"batchnormalization\"></a>[5] Ioffe, S.; Szegedy, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "<a id=\"dropout\"></a>[6] Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R. Dropout: A Simple Way to Prevent Neural Networks from\n",
    "Overfitting. Journal of Machine Learning Research, 15, 1929-1958, 2014 [http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n",
    "\n",
    "<a id=\"batchnormalization_and_dropout\"></a>[7] Li, X.; Chen, S.; Hu, X.; Yang, J. Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift [https://arxiv.org/abs/1801.05134](https://arxiv.org/abs/1801.05134)\n",
    "\n",
    "<a id=\"all_conv\"></a>[8] Springenberg, J. T.; Dosovitskiy, A.; Brox, T.; Riedmiller, M. Striving for Simplicity: The All Convolutional Net\n",
    "[https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806)\n",
    "\n",
    "<a id=\"googlenet\"></a>[9] Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; Rabinovich, A. Going Deeper with Convolutions\n",
    "[https://arxiv.org/pdf/1409.4842.pdf](https://arxiv.org/pdf/1409.4842.pdf)\n",
    "\n",
    "<a id=\"resnet\"></a>[10] He, K.; Zhang, X.; Ren, S.; Sun, J.;\n",
    "Deep Residual Learning for Image Recognition\n",
    "[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "<a id=\"smallminibatches\"></a>[11] Masters, D.; Luschi, C.; Revisiting Small Batch Training for Deep Neural Networks\n",
    "[https://arxiv.org/abs/1804.07612](https://arxiv.org/abs/1804.07612)\n",
    "\n",
    "<a id=\"radam\"></a>[14] Liu, L., Jinag; H., Pengcheng He, W.; Xiaodong Liu, J.; Han, J.; On the Variance of the Adaptive Learning Rate and Beyond [https://arxiv.org/pdf/1908.03265.pdf](https://arxiv.org/pdf/1908.03265.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
